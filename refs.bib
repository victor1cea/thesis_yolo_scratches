@misc{coco_site,
  author = {},
  title = {{Common Objects in Context}},
  howpublished = "\url{cocodataset.org}",
  note = {Accesed on 20.07.2022}
}

@misc{imagenet_site,
  author = {},
  title = {{ImageNet}},
  howpublished = "\url{https://image-net.org/}",
  note = {Accesed on 20.07.2022}
}

@misc{openimages_site,
  author = {},
  title = {{OpenImages}},
  howpublished = "\url{https://storage.googleapis.com/openimages/web/index.html}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov5_train_tips,
  author = {ultralytics},
  title = {{Tips for Best Training Results}},
  howpublished = "\url{https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov5_train_custom,
  author = {ultralytics},
  title = {{Train Custom Data}},
  howpublished = "\url{https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data}",
  note = {Accesed on 20.07.2022}
}

@misc{darknet_git,
  author = {AlexeyAB},
  title = {{Darknet}},
  howpublished = "\url{https://github.com/AlexeyAB/darknet}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov3_ultralytics_git,
  author = {ultralytics},
  title = {{YOLOv5}},
  howpublished = "\url{https://github.com/ultralytics/yolov3/}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov5_git,
  author = {ultralytics},
  title = {{YOLOv5}},
  howpublished = "\url{https://github.com/ultralytics/yolov5/}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov1_paper,
  doi = {10.48550/ARXIV.1506.02640},

  url = {https://arxiv.org/abs/1506.02640},

  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {You Only Look Once: Unified, Real-Time Object Detection},

  publisher = {arXiv},

  year = {2015},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yolov2_paper,
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={YOLO9000: Better, Faster, Stronger},
  year={2017},
  volume={},
  number={},
  pages={6517-6525},
  doi={10.1109/CVPR.2017.690}
       }

@misc{yolov3_paper,
  doi = {10.48550/ARXIV.1804.02767},

  url = {https://arxiv.org/abs/1804.02767},

  author = {Redmon, Joseph and Farhadi, Ali},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {YOLOv3: An Incremental Improvement},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{yolov4_paper,
  author    = {Alexey Bochkovskiy and
               Chien{-}Yao Wang and
               Hong{-}Yuan Mark Liao},
  title     = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  journal   = {CoRR},
  volume    = {abs/2004.10934},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.10934},
  eprinttype = {arXiv},
  eprint    = {2004.10934},
  timestamp = {Tue, 28 Apr 2020 16:10:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-10934.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{yolov7_paper,
  doi = {10.48550/ARXIV.2207.02696},

  url = {https://arxiv.org/abs/2207.02696},

  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yolor_paper,
  doi = {10.48550/ARXIV.2105.04206},

  url = {https://arxiv.org/abs/2105.04206},

  author = {Wang, Chien-Yao and Yeh, I-Hau and Liao, Hong-Yuan Mark},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {You Only Learn One Representation: Unified Network for Multiple Tasks},

  publisher = {arXiv},

  year = {2021},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yolox_paper,
  doi = {10.48550/ARXIV.2107.08430},

  url = {https://arxiv.org/abs/2107.08430},

  author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {YOLOX: Exceeding YOLO Series in 2021},

  publisher = {arXiv},

  year = {2021},

  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{map_tutorial,
  author = {Ren Jie Tan},
  title = {{Breaking Down Mean Average Precision (mAP)}},
  howpublished = "\url{https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov5_conf,
  author = {},
  title = {{YOLOv5 Github Issue \#8143}},
  howpublished = "\url{https://github.com/ultralytics/yolov5/issues/8143}",
  note = {Accesed on 20.07.2022}
}

@misc{yolov5_arch,
author = {},
title = {{YOLOv5 Github Issue \#8143}},
howpublished = "\url{https://github.com/ultralytics/yolov5/issues/280}",
note = {Accesed on 20.07.2022}
}

@misc{yolov3_vs,
authors = {Min Li, Zhijie Zhang, Liping Lei, Xiaofan Wang, Xudong Guo},
title = {{Agricultural Greenhouses Detection in High-Resolution Satellite}},
note = {Accesed on 20.07.2022}
}

@misc{yolo_compare,
author = {Upesh Nepal, Hossein Eslamiat},
title = {{Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs}},
note = {Accesed on 20.07.2022}
}

@misc{yolov5_focus,
  author = {},
  title = {{YOLOv5 Github Issue \#8143}},
  howpublished = "\url{https://github.com/ultralytics/yolov5/discussions/3181}",
  note = {Accesed on 20.07.2022}
}


@incollection{spp_paper,
	doi = {10.1007/978-3-319-10578-9_23},

	url = {https://doi.org/10.1007%2F978-3-319-10578-9_23},

	year = 2014,
	publisher = {Springer International Publishing},

	pages = {346--361},

	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},

	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},

	booktitle = {Computer Vision {\textendash} {ECCV} 2014}
}

@misc{fpn_paper,
  doi = {10.48550/ARXIV.1612.03144},

  url = {https://arxiv.org/abs/1612.03144},

  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Feature Pyramid Networks for Object Detection},

  publisher = {arXiv},

  year = {2016},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{panet_paper,
  doi = {10.48550/ARXIV.1803.01534},

  url = {https://arxiv.org/abs/1803.01534},

  author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Path Aggregation Network for Instance Segmentation},

  publisher = {arXiv},

  year = {2018},

  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{fiftyone_git,
  author = {},
  title = {{FiftyOne Github}},
  howpublished = "\url{https://github.com/voxel51/fiftyone}",
  note = {Accesed on 20.07.2022}
}

@misc{cvat_git,
  author = {},
  title = {{CVAT Github}},
  howpublished = "\url{https://github.com/openvinotoolkit/cvat}",
  note = {Accesed on 20.07.2022}
}

@misc{albumentations_site,
  author = {},
  title = {{Albumentations}},
  howpublished = "\url{https://albumentations.ai/}",
  note = {Accesed on 20.07.2022}
}

@misc{wandb_site,
  author = {},
  title = {{Weights \& Biases}},
  howpublished = "\url{https://wandb.ai/}",
  note = {Accesed on 20.07.2022}
}

@misc{ultralytics_site,
  author = {},
  title = {{Ultralytics}},
  howpublished = "\url{https://https://ultralytics.com/}",
  note = {Accesed on 20.07.2022}
}


@misc{binder_jetting,
  author = {},
  title = {{What is Binder Jetting 3D printing?}},
  howpublished = "\url{https://www.hubs.com/knowledge-base/introduction-binder-jetting-3d-printing/}",
  note = {Accesed on 20.07.2022}
}


%before YOLO

@misc{dpm_paper,
  author={Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Object Detection with Discriminatively Trained Part-Based Models},
  year={2010},
  volume={32},
  number={9},
  pages={1627-1645},
  doi={10.1109/TPAMI.2009.167}
}


@article{rcnn_selective_search_paper,
author = {Uijlings, J. R. and Sande, K. E. and Gevers, T. and Smeulders, A. W.},
title = {Selective Search for Object Recognition},
year = {2013},
issue_date = {September 2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-013-0620-5},
doi = {10.1007/s11263-013-0620-5},
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).},
journal = {Int. J. Comput. Vision},
month = {sep},
pages = {154–171},
numpages = {18}
}

@misc{fast_rcnn_paper,
  doi = {10.48550/ARXIV.1504.08083},
  url = {https://arxiv.org/abs/1504.08083},
  author = {Girshick, Ross},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fast R-CNN},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{faster_rcnn_paper,
  doi = {10.48550/ARXIV.1506.01497},
  url = {https://arxiv.org/abs/1506.01497},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{megengine_git,
  author = {},
  title = {{MegEngine Github}},
  howpublished = "\url{https://github.com/MegEngine/MegEngine}",
  note = {Accesed on 20.07.2022}
}

@misc{roboflow_yolov4_augs,
  author = {},
  title = {{Data Augmentation in YOLOv4}},
  howpublished = "\url{https://blog.roboflow.com/yolov4-data-augmentation/}",
  note = {Accesed on 20.07.2022}
}

@misc{viola_joines_paper,
  author={Viola, P. and Jones, M.},
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
  title={Rapid object detection using a boosted cascade of simple features},
  year={2001},
  volume={1},
  number={},
  pages={I-I},
  doi={10.1109/CVPR.2001.990517}
  }


  @Article{one_stage_vs_two_stage_paper,
  AUTHOR = {Carranza-García, Manuel and Torres-Mateo, Jesús and Lara-Benítez, Pedro and García-Gutiérrez, Jorge},
  TITLE = {On the Performance of One-Stage and Two-Stage Object Detectors in Autonomous Vehicles Using Camera Data},
  JOURNAL = {Remote Sensing},
  VOLUME = {13},
  YEAR = {2021},
  NUMBER = {1},
  ARTICLE-NUMBER = {89},
  URL = {https://www.mdpi.com/2072-4292/13/1/89},
  ISSN = {2072-4292},
  ABSTRACT = {Object detection using remote sensing data is a key task of the perception systems of self-driving vehicles. While many generic deep learning architectures have been proposed for this problem, there is little guidance on their suitability when using them in a particular scenario such as autonomous driving. In this work, we aim to assess the performance of existing 2D detection systems on a multi-class problem (vehicles, pedestrians, and cyclists) with images obtained from the on-board camera sensors of a car. We evaluate several one-stage (RetinaNet, FCOS, and YOLOv3) and two-stage (Faster R-CNN) deep learning meta-architectures under different image resolutions and feature extractors (ResNet, ResNeXt, Res2Net, DarkNet, and MobileNet). These models are trained using transfer learning and compared in terms of both precision and efficiency, with special attention to the real-time requirements of this context. For the experimental study, we use the Waymo Open Dataset, which is the largest existing benchmark. Despite the rising popularity of one-stage detectors, our findings show that two-stage detectors still provide the most robust performance. Faster R-CNN models outperform one-stage detectors in accuracy, being also more reliable in the detection of minority classes. Faster R-CNN Res2Net-101 achieves the best speed/accuracy tradeoff but needs lower resolution images to reach real-time speed. Furthermore, the anchor-free FCOS detector is a slightly faster alternative to RetinaNet, with similar precision and lower memory usage.},
  DOI = {10.3390/rs13010089}
  }

  @misc{focal_loss_paper,
  doi = {10.48550/ARXIV.1708.02002},
  url = {https://arxiv.org/abs/1708.02002},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Focal Loss for Dense Object Detection},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{improved_mosaic_paper,
author = {Wang Hao and Song Zhili },
title = {Improved Mosaic: Algorithms for more ComplexImages},
}

@misc{yolo_smoke_paper,
author = {Zhong Wang , Lei Wu, Tong Li * and Peibei Shi},
title = {A Smoke Detection Model Based on Improved YOLOv5},
}

@misc{googlenet_paper,
  doi = {10.48550/ARXIV.1409.4842},

  url = {https://arxiv.org/abs/1409.4842},

  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Going Deeper with Convolutions},

  publisher = {arXiv},

  year = {2014},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{scipy_signal,
  author = {},
  title = {{Scipy - Signal processing module}},
  howpublished = "\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html}",
  note = {Accesed on 20.07.2022}
}
