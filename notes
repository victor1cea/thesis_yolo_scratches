beispiel bild
object detection neural network overview, unet deep learning, si cauta un survey paper
yolov1-4 e facut de ala [paper1, ... paper4], dar yolo5 de[...] si e etabliert, e si in opencv etc.
deseneaza workflow si arata si alternative (CVAT vs fiftyone de exemplu) si sa discut in


1ch e echivalent cu 3ch

gcloud notebooks instances create example-instance4 \
    --vm-image-project=deeplearning-platform-release \
    --vm-image-family=pytorch-1-11-cu113-notebooks \
    --machine-type=n1-standard-4 \
    --location=europe-west1-b \
    --boot-disk-size=100 \
    --accelerator-core-count=1 \
    --accelerator-type=NVIDIA_TESLA_T4 \
    --install-gpu-driver \
    --network=default \
    --project=machine-learning-vo

gcloud notebooks instances create vo_instance2 \
    --vm-image-project=deeplearning-platform-release \
    --vm-image-family=pytorch-1-11-cu113-notebooks \
    --machine-type=n1-standard-4 \
    --location=europe-west1-b \
    --boot-disk-size=100 \
    --accelerator-core-count=1 \
    --accelerator-type=NVIDIA_TESLA_P100 \
    --install-gpu-driver \
    --network=default \
    --project=machine-learning-vo


python -m torch.distributed.launch --nproc_per_node 2  yolov5/train.py --data /home/captain/ds_double/dataset.yaml --name exp_ds_blur_sobel_tv_mix --project tv_mix_bitmask_methods --weights yolov5m6.pt --epochs 50 --hyp yolov5/data/hyps/hyp.ref.yaml --batch-size 64 --imgsz 640 --optimizer AdamW --save-period 20 --patience 75

layer_823: scratch e de fapt pe layer
layer_843: scratch e fix pe muchie dar mai jos se distanteaza
layer_xxx (265 cred): scratch nu e de fapt pe layer
layer_1024: thin spaces might look like scratches, humans can t tell
layer_1291: scratch e fix pe thin space din prev bitmask
layer_1502: not scratches, thin spaces from previous layer, very nice bm_prev
layer_2206: de exemplu acolo am ratat
layer_1259, layer_1950: aici thin space from next layer actually

testeaza ipoteza ca pui atele deasupra si sub bucata si iti creste conf sau generezi un

228 layers
ds -> train s, val s, inf s, inf im count, gen_time/300 layers
1280 -> 93 s, 9 s, 82 s, 1368 im, 11 s
960 -> 36 s, 4 s, 47 s, 1368 im, 11 s
640 -> 32 s, 3 s, 67 s, 3420 im, 10 s
320 -> 14 s, 2 s, 105 s, 10260 im, 10 s


\begin{figure}[!h]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[width=\columnwidth]{TODO}
\caption{TODO}
\label{TODO}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{subim 1}
  \caption{subcap 1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{subim2}
  \caption{subcap 2}
\end{subfigure}
\caption{caption}
\label{TODO}
\end{figure}


\begin{table}
  \centering
    \begin{tabular}{ ||c|c|c||}
    \hline
    window size & seconds/epoch & total windows\\ [0.5ex]
    \hline\hline
    320x320 & 26 & 3328 \\
    640x640 & 35 & 1777 \\
    960x960 & 44 & 887 \\
    1280x1280 & 102 & 1214 \\
    \hline
    \end{tabular}
  \label{win_size_training}
  \caption{Relevant stats for training on different windows size.}
\end{table}

conclusions pt urmatorul student

23 + 250 + 22

lista de experimente:
full image pt windowing
fara mosaic
learning rate prost
batch size
